# Speculative Decoding: Implementation Blueprint (Phase 1â€“4)

> **ëª©ì **: ì´ ë¬¸ì„œëŠ” Claude Codeê°€ ì½”ë“œ ì‘ì„± í”Œëœì„ ìˆ˜ë¦½í•˜ê³  êµ¬í˜„ì„ ì‹œì‘í•˜ê¸° ìœ„í•œ ì™„ì „í•œ ì°¸ì¡° ë¬¸ì„œì…ë‹ˆë‹¤.
> mlx-lm ìœ„ì˜ ì»¤ìŠ¤í…€ API ë ˆì´ì–´ì— speculative decodingì„ í†µí•©í•©ë‹ˆë‹¤.
> Continuous batchingì€ ì´ë¯¸ êµ¬í˜„ ì™„ë£Œëœ ìƒíƒœì…ë‹ˆë‹¤.

---

## 0. ì „ì œ ì¡°ê±´ ë° í˜„ì¬ ìƒíƒœ

| Component | Status | ë¹„ê³  |
|-----------|--------|------|
| OpenAI-compat API Layer | âœ… Done | FastAPI ê¸°ë°˜ |
| Continuous Batching Engine | âœ… Done | vLLM-MLX ë¡œì§ ì´ì‹, token-level batched forward |
| Speculative Decoding | ğŸ”§ êµ¬í˜„ ëŒ€ìƒ | ì´ ë¬¸ì„œì˜ ë²”ìœ„ |

**ê¸°ìˆ  ìŠ¤íƒ**: Python 3.12+, mlx, mlx-lm, Apple Silicon (M-series), Metal

**í•µì‹¬ ì„¤ê³„ ì›ì¹™**:
- `--spec-decode={none|ngram|draft|eagle}` flagë¡œ ëª¨ë“œ ì„ íƒ
- ProposerëŠ” Strategy Patternìœ¼ë¡œ êµì²´ ê°€ëŠ¥
- ê¸°ì¡´ continuous batching ì—”ì§„ì˜ `step()` ë£¨í”„ì— ìì—°ìŠ¤ëŸ½ê²Œ ì‚½ì…
- Dynamic controlë¡œ ë°°ì¹˜ í¬ê¸°ì— ë”°ë¼ ìë™ ON/OFF

---

## 1. íŒŒì¼ êµ¬ì¡°

```
hwquant/
â”œâ”€â”€ serve.py                          # CLI entrypoint (ê¸°ì¡´)
â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ engine.py                     # ë©”ì¸ ì—”ì§„ ë£¨í”„ (ê¸°ì¡´, ìˆ˜ì •)
â”‚   â”œâ”€â”€ scheduler.py                  # Unified scheduler (ê¸°ì¡´, ìˆ˜ì •)
â”‚   â””â”€â”€ request.py                    # Request/Sequence ë°ì´í„° (ê¸°ì¡´)
â”œâ”€â”€ spec_decode/                      # â˜… ì‹ ê·œ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                     # SpecDecodeConfig
â”‚   â”œâ”€â”€ proposer/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py                   # BaseProposer (ABC)
â”‚   â”‚   â”œâ”€â”€ ngram.py                  # Phase 1: NGramProposer
â”‚   â”‚   â”œâ”€â”€ draft_model.py            # Phase 2: DraftModelProposer
â”‚   â”‚   â””â”€â”€ eagle.py                  # Phase 3: EAGLEProposer
â”‚   â”œâ”€â”€ verifier.py                   # BatchedVerifier
â”‚   â”œâ”€â”€ rejection_sampler.py          # BatchedRejectionSampler
â”‚   â”œâ”€â”€ dynamic_controller.py         # Phase 4: DynamicSpecController
â”‚   â””â”€â”€ kv_manager.py                 # SpecDecodeKVManager
â””â”€â”€ tests/
    â””â”€â”€ spec_decode/
        â”œâ”€â”€ test_ngram_proposer.py
        â”œâ”€â”€ test_rejection_sampler.py
        â”œâ”€â”€ test_draft_model.py
        â”œâ”€â”€ test_eagle.py
        â”œâ”€â”€ test_dynamic_controller.py
        â””â”€â”€ test_engine_integration.py
```

---

## 2. Config (spec_decode/config.py)

```python
from dataclasses import dataclass, field
from typing import Literal, Optional


@dataclass
class SpecDecodeConfig:
    """
    Speculative decoding ì „ì²´ ì„¤ì •.
    CLI arg, config file, API request-level override ëª¨ë‘ ì´ êµ¬ì¡°ì²´ë¡œ í†µí•©.
    """

    # â”€â”€â”€ ëª¨ë“œ ì„ íƒ â”€â”€â”€
    spec_decode_mode: Literal["none", "ngram", "draft", "eagle"] = "none"

    # â”€â”€â”€ ê³µí†µ â”€â”€â”€
    num_speculative_tokens: int = 5           # draft í† í° ìˆ˜ k
    disable_by_batch_size: int = 8            # batch >= ì´ ê°’ì´ë©´ ìë™ OFF
    acceptance_rate_threshold: float = 0.3    # EMAê°€ ì´ ë¯¸ë§Œì´ë©´ spec ì¤‘ë‹¨

    # â”€â”€â”€ Phase 1: N-gram â”€â”€â”€
    ngram_max: int = 4
    ngram_min: int = 1
    ngram_prompt_lookup: bool = True          # prompt í† í°ì—ì„œë„ ë§¤ì¹­ íƒìƒ‰

    # â”€â”€â”€ Phase 2: Draft Model â”€â”€â”€
    draft_model_path: Optional[str] = None
    draft_model_quantize: Optional[str] = None  # e.g. "4bit", "8bit"

    # â”€â”€â”€ Phase 3: EAGLE â”€â”€â”€
    eagle_head_path: Optional[str] = None     # ì¶”ê°€ prediction head ê²½ë¡œ
    eagle_num_layers: int = 1                 # prediction head ë ˆì´ì–´ ìˆ˜

    # â”€â”€â”€ Phase 4: Dynamic Control â”€â”€â”€
    dynamic_spec_decode: bool = True
    acceptance_rate_ema_alpha: float = 0.1    # EMA smoothing factor
    adaptive_k: bool = True                   # acceptance rateì— ë”°ë¼ k ë™ì  ì¡°ì ˆ

    def validate(self):
        if self.spec_decode_mode == "draft" and not self.draft_model_path:
            raise ValueError("--draft-model required when --spec-decode=draft")
        if self.spec_decode_mode == "eagle" and not self.eagle_head_path:
            raise ValueError("--eagle-head-path required when --spec-decode=eagle")
```

**CLI ë§¤í•‘** (serve.pyì— ì¶”ê°€):

```python
# argparse ê·¸ë£¹
spec_group = parser.add_argument_group("Speculative Decoding")
spec_group.add_argument("--spec-decode", choices=["none","ngram","draft","eagle"], default="none")
spec_group.add_argument("--num-speculative-tokens", type=int, default=5)
spec_group.add_argument("--disable-by-batch-size", type=int, default=8)
spec_group.add_argument("--ngram-max", type=int, default=4)
spec_group.add_argument("--ngram-min", type=int, default=1)
spec_group.add_argument("--ngram-prompt-lookup", action="store_true", default=True)
spec_group.add_argument("--draft-model", type=str, default=None)
spec_group.add_argument("--draft-model-quantize", type=str, default=None)
spec_group.add_argument("--eagle-head-path", type=str, default=None)
spec_group.add_argument("--dynamic-spec-decode", action="store_true", default=True)
spec_group.add_argument("--no-dynamic-spec-decode", dest="dynamic_spec_decode", action="store_false")
```

**Request-level override** (OpenAI extra_body):

```json
{
    "model": "qwen3-32b",
    "messages": [],
    "extra_body": {
        "spec_decode": "ngram",
        "num_speculative_tokens": 3
    }
}
```

---

## 3. Proposer Interface (spec_decode/proposer/base.py)

ëª¨ë“  proposerê°€ êµ¬í˜„í•˜ëŠ” í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤. Strategy Patternìœ¼ë¡œ ì—”ì§„ì—ì„œ êµì²´ ê°€ëŠ¥.

```python
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import List, Optional, Tuple

import mlx.core as mx


@dataclass
class ProposalResult:
    """Proposerì˜ ì¶œë ¥."""
    draft_tokens: mx.array          # [batch, k] â€” proposalëœ í† í° ID
    draft_probs: Optional[mx.array] # [batch, k, vocab] â€” draft modelì¼ ë•Œë§Œ, n-gramì€ None
    proposal_lens: mx.array         # [batch] â€” ì‹œí€€ìŠ¤ë³„ ì‹¤ì œ proposal ê¸¸ì´ (íŒ¨ë”© ì œì™¸)


class BaseProposer(ABC):
    """ëª¨ë“  proposerì˜ ê¸°ë°˜ í´ë˜ìŠ¤."""

    @abstractmethod
    def propose(
        self,
        sequences: List["Sequence"],
        k: int,
    ) -> Optional[ProposalResult]:
        """
        ë°°ì¹˜ ë‚´ ëª¨ë“  ì‹œí€€ìŠ¤ì— ëŒ€í•´ draft token ìƒì„±.

        Args:
            sequences: decode ìƒíƒœì˜ ì‹œí€€ìŠ¤ ë¦¬ìŠ¤íŠ¸
            k: ì‹œí€€ìŠ¤ë‹¹ ìƒì„±í•  draft í† í° ìˆ˜

        Returns:
            ProposalResult ë˜ëŠ” None (ì „ì²´ ë°°ì¹˜ proposal ì‹¤íŒ¨ ì‹œ)
        """
        ...

    @property
    @abstractmethod
    def needs_draft_probs(self) -> bool:
        """
        True: rejection sampling ì‚¬ìš© (draft model, EAGLE)
        False: greedy/threshold verification ì‚¬ìš© (n-gram)
        """
        ...

    @property
    @abstractmethod
    def requires_gpu(self) -> bool:
        """Trueë©´ Metal GPU ì‚¬ìš© (draft model, EAGLE), Falseë©´ CPU (n-gram)"""
        ...


def create_proposer(config: "SpecDecodeConfig", target_model=None) -> Optional[BaseProposer]:
    """Factory function. config.spec_decode_modeì— ë”°ë¼ proposer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±."""
    match config.spec_decode_mode:
        case "none":
            return None
        case "ngram":
            from .ngram import NGramProposer
            return NGramProposer(
                ngram_max=config.ngram_max,
                ngram_min=config.ngram_min,
                prompt_lookup=config.ngram_prompt_lookup,
            )
        case "draft":
            from .draft_model import DraftModelProposer
            return DraftModelProposer(
                model_path=config.draft_model_path,
                quantize=config.draft_model_quantize,
            )
        case "eagle":
            from .eagle import EAGLEProposer
            return EAGLEProposer(
                target_model=target_model,
                head_path=config.eagle_head_path,
                num_layers=config.eagle_num_layers,
            )
```

---

## 4. Phase 1: N-gram Proposer (spec_decode/proposer/ngram.py)

### 4.1 ì„¤ê³„ ê·¼ê±°

- vLLMì˜ `vllm/spec_decode/ngram_worker.py` íŒ¨í„´ ì´ì‹
- ì¶”ê°€ ëª¨ë¸ ë¡œë”© ì—†ìŒ â†’ ë©”ëª¨ë¦¬ ì˜¤ë²„í—¤ë“œ 0
- CPUì—ì„œ ìˆœìˆ˜ Python ì‹¤í–‰ â†’ Metal GPUëŠ” target modelì— ì „ë…
- ì½”ë“œ ìƒì„±, ë²ˆì—­, ìš”ì•½ ë“± ë°˜ë³µ íŒ¨í„´ì´ ë§ì€ taskì—ì„œ ë†’ì€ acceptance rate

### 4.2 êµ¬í˜„

```python
from typing import Dict, List, Optional, Tuple

import mlx.core as mx

from .base import BaseProposer, ProposalResult


class NGramProposer(BaseProposer):
    """
    Context(prompt + generated) ë‚´ n-gram ë§¤ì¹­ìœ¼ë¡œ draft token ìƒì„±.

    ì•Œê³ ë¦¬ì¦˜:
    1. í˜„ì¬ context ë nê°œ í† í°ì„ keyë¡œ ì„¤ì •
    2. context ì•ë¶€ë¶„ì—ì„œ ê°™ì€ n-gram íƒìƒ‰
    3. ë§¤ì¹­ ì‹œ ê·¸ ë’¤ kê°œ í† í°ì„ draftë¡œ ì œì•ˆ
    4. í° në¶€í„° íƒìƒ‰ (4-gram â†’ 3-gram â†’ ... â†’ 1-gram)

    vLLM ngram_worker ëŒ€ë¹„ ì°¨ì´:
    - Batched: ì—¬ëŸ¬ ì‹œí€€ìŠ¤ì— ëŒ€í•´ í•œ ë²ˆì— proposal
    - suffix index ìºì‹±ìœ¼ë¡œ ê¸´ contextì—ì„œ O(1) íƒìƒ‰
    - ë§¤ì¹­ ì‹¤íŒ¨ ì‹œí€€ìŠ¤ëŠ” proposal_len=0ìœ¼ë¡œ í‘œì‹œ â†’ í•´ë‹¹ ì‹œí€€ìŠ¤ë§Œ ì¼ë°˜ decode
    """

    def __init__(self, ngram_max: int = 4, ngram_min: int = 1,
                 prompt_lookup: bool = True):
        self.ngram_max = ngram_max
        self.ngram_min = ngram_min
        self.prompt_lookup = prompt_lookup

    @property
    def needs_draft_probs(self) -> bool:
        return False

    @property
    def requires_gpu(self) -> bool:
        return False  # ìˆœìˆ˜ CPU ì—°ì‚°

    def propose(
        self,
        sequences: List["Sequence"],
        k: int,
    ) -> Optional[ProposalResult]:
        batch_proposals: List[List[int]] = []
        proposal_lens: List[int] = []
        any_found = False

        for seq in sequences:
            tokens = self._propose_single(seq, k)
            batch_proposals.append(tokens)
            proposal_lens.append(len(tokens))
            if tokens:
                any_found = True

        if not any_found:
            return None  # ì „ì²´ ë°°ì¹˜ ì‹¤íŒ¨ â†’ ì¼ë°˜ decode fallback

        # ê°€ì¥ ê¸´ proposalì— ë§ì¶° 0-padding
        max_len = max(proposal_lens) if proposal_lens else 0
        if max_len == 0:
            return None

        padded = []
        for p in batch_proposals:
            padded.append(p + [0] * (max_len - len(p)))

        return ProposalResult(
            draft_tokens=mx.array(padded, dtype=mx.int32),
            draft_probs=None,
            proposal_lens=mx.array(proposal_lens, dtype=mx.int32),
        )

    def _propose_single(self, seq: "Sequence", k: int) -> List[int]:
        """
        ë‹¨ì¼ ì‹œí€€ìŠ¤ì— ëŒ€í•œ n-gram ë§¤ì¹­.

        í° n-gramë¶€í„° íƒìƒ‰ â†’ ì²« ë§¤ì¹­ì—ì„œ ë‹¤ìŒ kê°œ í† í° ë°˜í™˜.
        suffix indexê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì„ í˜• íƒìƒ‰.
        """
        if self.prompt_lookup:
            context = seq.prompt_tokens + seq.generated_tokens
        else:
            context = seq.generated_tokens

        if len(context) < self.ngram_min + 1:
            return []

        # suffix index ì‚¬ìš© (ìºì‹±)
        if hasattr(seq, '_ngram_index') and not seq._ngram_dirty:
            return self._propose_with_index(seq._ngram_index, context, k)

        # ì„ í˜• íƒìƒ‰ fallback
        return self._propose_linear(context, k)

    def _propose_linear(self, context: List[int], k: int) -> List[int]:
        """ì„ í˜• íƒìƒ‰. contextê°€ ì§§ì„ ë•Œ (< ~1000 í† í°) ì¶©ë¶„íˆ ë¹ ë¦„."""
        for n in range(self.ngram_max, self.ngram_min - 1, -1):
            if len(context) < n + 1:
                continue

            key = tuple(context[-n:])
            search_end = len(context) - n  # í˜„ì¬ ìœ„ì¹˜ ìì‹ ì€ ì œì™¸

            # ì—­ìˆœ íƒìƒ‰: ìµœê·¼ ë§¤ì¹­ì´ ë” ê´€ë ¨ì„± ë†’ìŒ
            for i in range(search_end - 1, -1, -1):
                if tuple(context[i:i + n]) == key:
                    start = i + n
                    end = min(start + k, len(context))
                    proposals = list(context[start:end])
                    if len(proposals) >= 1:
                        return proposals[:k]

        return []

    def _propose_with_index(
        self, index: Dict[tuple, List[int]], context: List[int], k: int
    ) -> List[int]:
        """
        Suffix index ê¸°ë°˜ O(1) íƒìƒ‰.
        indexëŠ” {n-gram_tuple: [position_list]} í˜•íƒœ.
        """
        for n in range(self.ngram_max, self.ngram_min - 1, -1):
            if len(context) < n + 1:
                continue

            key = tuple(context[-n:])
            if key not in index:
                continue

            positions = index[key]
            # ì—­ìˆœ: ë§ˆì§€ë§‰(ìµœê·¼) ìœ„ì¹˜ë¶€í„° íƒìƒ‰
            for pos in reversed(positions):
                if pos + n >= len(context):
                    continue  # í˜„ì¬ ìœ„ì¹˜ ìì‹  ì œì™¸
                start = pos + n
                end = min(start + k, len(context))
                proposals = list(context[start:end])
                if len(proposals) >= 1:
                    return proposals[:k]

        return []

    @staticmethod
    def build_suffix_index(tokens: List[int], ngram_max: int = 4) -> Dict[tuple, List[int]]:
        """
        ì‹œí€€ìŠ¤ì— ëŒ€í•œ n-gram suffix index êµ¬ì¶•.
        í˜¸ì¶œ ì‹œì : prefill ì™„ë£Œ í›„ 1íšŒ, ì´í›„ ìƒì„± í† í° ì¶”ê°€ ì‹œ incremental update.

        Returns:
            {(token_1, ..., token_n): [pos_0, pos_1, ...]}
        """
        index: Dict[tuple, List[int]] = {}
        for n in range(1, ngram_max + 1):
            for i in range(len(tokens) - n):
                key = tuple(tokens[i:i + n])
                if key not in index:
                    index[key] = []
                index[key].append(i)
        return index

    @staticmethod
    def update_suffix_index(
        index: Dict[tuple, List[int]],
        tokens: List[int],
        new_token_count: int,
        ngram_max: int = 4,
    ):
        """
        ìƒˆ í† í° ì¶”ê°€ ì‹œ incremental index ì—…ë°ì´íŠ¸.
        ì „ì²´ ì¬êµ¬ì¶• ëŒ€ì‹  ìƒˆë¡œ ì¶”ê°€ëœ ì˜ì—­ë§Œ ê°±ì‹ .
        """
        start = max(0, len(tokens) - new_token_count - ngram_max)
        for n in range(1, ngram_max + 1):
            for i in range(start, len(tokens) - n):
                key = tuple(tokens[i:i + n])
                if key not in index:
                    index[key] = []
                if not index[key] or index[key][-1] != i:
                    index[key].append(i)
```

### 4.3 í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ (tests/spec_decode/test_ngram_proposer.py)

```python
def test_exact_match():
    """ë°˜ë³µ íŒ¨í„´ì´ ìˆìœ¼ë©´ ì •í™•íˆ proposalí•´ì•¼ í•¨."""
    # context: "A B C D E A B C D E F G"
    # í˜„ì¬ ëì´ [D, E] â†’ ì•ì—ì„œ [D, E] ë§¤ì¹­ â†’ [A, B, C] proposal (if k=3)
    seq = make_sequence(prompt=[1,2,3,4,5], generated=[1,2,3,4,5,6,7])
    proposer = NGramProposer(ngram_max=2)
    result = proposer._propose_single(seq, k=3)
    # context[-2:] = [6,7], ì•ì—ì„œ ë§¤ì¹­ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
    # context[-2:] = [4,5] ë‘ ë²ˆ ë‚˜íƒ€ë‚¨ â†’ ë‘ ë²ˆì§¸ ë’¤ì˜ [6,7] ë˜ëŠ” ì²« ë²ˆì§¸ ë’¤ì˜ [1,2,3]

def test_no_match():
    """ë§¤ì¹­ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜."""
    seq = make_sequence(prompt=[1,2,3], generated=[4,5,6])
    proposer = NGramProposer(ngram_max=4)
    result = proposer._propose_single(seq, k=3)
    assert result == []

def test_batch_partial_match():
    """ë°°ì¹˜ ë‚´ ì¼ë¶€ë§Œ ë§¤ì¹­ë˜ë©´, ë§¤ì¹­ëœ ê²ƒë§Œ proposal ìˆì–´ì•¼ í•¨."""
    proposer = NGramProposer(ngram_max=2)
    seqs = [make_sequence_with_repeat(), make_sequence_without_repeat()]
    result = proposer.propose(seqs, k=3)
    assert result is not None
    assert result.proposal_lens[0] > 0
    assert result.proposal_lens[1] == 0

def test_suffix_index_correctness():
    """Suffix indexê°€ ì„ í˜• íƒìƒ‰ê³¼ ë™ì¼í•œ ê²°ê³¼ë¥¼ ë‚´ì•¼ í•¨."""
    tokens = [1,2,3,4,5,1,2,3,4,5,6]
    index = NGramProposer.build_suffix_index(tokens, ngram_max=4)
    # ì„ í˜• íƒìƒ‰ê³¼ index íƒìƒ‰ ê²°ê³¼ ë¹„êµ
```

---

## 5. Batched Verifier (spec_decode/verifier.py)

Draft tokenë“¤ì„ target modelë¡œ ì¼ê´„ ê²€ì¦í•˜ëŠ” ëª¨ë“ˆ. ëª¨ë“  Phaseì—ì„œ ê³µìœ .

```python
from typing import List

import mlx.core as mx


class BatchedVerifier:
    """
    Target modelë¡œ draft token ì¼ê´„ ê²€ì¦.

    CUDAì™€ ë‹¬ë¦¬ batch expansion(ì‹œí€€ìŠ¤ ë³µì œ) ì—†ì´
    padding + attention maskë¡œ ê°€ë³€ ê¸¸ì´ ì²˜ë¦¬.
    Apple Silicon unified memory â†’ zero-copy.
    """

    def __init__(self, target_model, tokenizer):
        self.model = target_model
        self.tokenizer = tokenizer

    def verify(
        self,
        sequences: List["Sequence"],
        draft_tokens: mx.array,       # [batch, k]
        proposal_lens: mx.array,      # [batch] â€” ì‹œí€€ìŠ¤ë³„ ì‹¤ì œ proposal ê¸¸ì´
    ) -> mx.array:
        """
        Target model forward passë¡œ draft tokenë“¤ì˜ í™•ë¥  ë¶„í¬ ê³„ì‚°.

        ê° ì‹œí€€ìŠ¤ì— ëŒ€í•´ context ë í† í° + draft tokensë¥¼ ì…ë ¥ìœ¼ë¡œ ë„£ê³ ,
        single forward passë¡œ target_probsë¥¼ ì–»ìŒ.

        Args:
            sequences: decode ì¤‘ì¸ ì‹œí€€ìŠ¤ ë¦¬ìŠ¤íŠ¸
            draft_tokens: [batch, k] â€” proposerê°€ ìƒì„±í•œ draft
            proposal_lens: [batch] â€” ì‹œí€€ìŠ¤ë³„ ìœ íš¨í•œ proposal ê¸¸ì´

        Returns:
            target_probs: [batch, max_k+1, vocab_size]
            max_k+1ì¸ ì´ìœ : kê°œ draft token verification + 1ê°œ bonus/resample ìœ„ì¹˜
        """
        batch_size = len(sequences)
        k = draft_tokens.shape[1]

        # ê° ì‹œí€€ìŠ¤ë³„ verification ì…ë ¥ êµ¬ì„±
        # ì…ë ¥ = [last_token_of_context, draft_token_0, ..., draft_token_{k-1}]
        # â†’ target modelì´ ê° ìœ„ì¹˜ì—ì„œì˜ next-token í™•ë¥ ì„ ì¶œë ¥
        verify_inputs = []
        for i, seq in enumerate(sequences):
            plen = int(proposal_lens[i])
            if plen == 0:
                # proposal ì—†ëŠ” ì‹œí€€ìŠ¤: ì¼ë°˜ decode (1 í† í°ë§Œ)
                verify_inputs.append(mx.array([seq.last_token_id], dtype=mx.int32))
            else:
                # last_token + draft_tokens[:plen]
                tokens = mx.concatenate([
                    mx.array([seq.last_token_id], dtype=mx.int32),
                    draft_tokens[i, :plen],
                ])
                verify_inputs.append(tokens)

        # Padding + batching
        max_len = max(v.shape[0] for v in verify_inputs)
        padded_input = mx.zeros((batch_size, max_len), dtype=mx.int32)
        attention_mask = mx.zeros((batch_size, max_len), dtype=mx.bool_)

        for i, v in enumerate(verify_inputs):
            padded_input[i, :v.shape[0]] = v
            attention_mask[i, :v.shape[0]] = True

        # â˜… Single target model forward pass (ë°°ì¹˜ ì „ì²´)
        # ê¸°ì¡´ continuous batching ì—”ì§„ì˜ forward ë©”ì„œë“œ í™œìš©
        # KV cacheëŠ” ê° ì‹œí€€ìŠ¤ì˜ ê¸°ì¡´ cacheë¥¼ extendí•˜ëŠ” ë°©ì‹
        target_logits = self._forward_with_kv_cache(
            sequences, padded_input, attention_mask
        )
        # target_logits: [batch, max_len, vocab_size]

        target_probs = mx.softmax(target_logits, axis=-1)
        mx.eval(target_probs)

        return target_probs

    def _forward_with_kv_cache(self, sequences, input_ids, mask):
        """
        ê¸°ì¡´ continuous batching ì—”ì§„ì˜ batched forwardë¥¼ í˜¸ì¶œ.
        ê° ì‹œí€€ìŠ¤ì˜ KV cacheë¥¼ extendí•˜ë©´ì„œ forward pass ìˆ˜í–‰.

        â˜… êµ¬í˜„ ì‹œ ê¸°ì¡´ ì—”ì§„ì˜ forward ì¸í„°í˜ì´ìŠ¤ì— ë§ì¶° ì¡°ì • í•„ìš”.
        """
        # TODO: ê¸°ì¡´ ì—”ì§„ì˜ batched forward ë©”ì„œë“œì— ìœ„ì„
        # í•µì‹¬: input_idsê°€ 1ê°œ(ì¼ë°˜ decode)ê°€ ì•„ë‹ˆë¼ k+1ê°œì´ë¯€ë¡œ
        #        KV cacheë¥¼ k+1 ìœ„ì¹˜ë§Œí¼ extend
        raise NotImplementedError("ê¸°ì¡´ continuous batching ì—”ì§„ì˜ forwardì— ìœ„ì„")
```

**êµ¬í˜„ ì‹œ ì£¼ì˜ì **:
- ê¸°ì¡´ continuous batching ì—”ì§„ì˜ forward passëŠ” `input_ids`ê°€ ì‹œí€€ìŠ¤ë‹¹ 1ê°œ í† í°ì„ ê°€ì •í•  ìˆ˜ ìˆìŒ
- Spec decode ì‹œì—ëŠ” ì‹œí€€ìŠ¤ë‹¹ `k+1`ê°œ í† í°ì„ ì…ë ¥í•˜ë¯€ë¡œ, forwardì— ê°€ë³€ ê¸¸ì´ ì…ë ¥ì„ ë°›ì„ ìˆ˜ ìˆë„ë¡ í™•ì¥ í•„ìš”
- KV cacheë„ `k+1` ìœ„ì¹˜ë§Œí¼ pre-allocate â†’ reject ì‹œ truncate

---

## 6. Rejection Sampler (spec_decode/rejection_sampler.py)

### 6.1 í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ (vLLM V1 ê¸°ë°˜)

```
For each sequence in batch:
    For position i = 0, 1, ..., k-1 (left to right):
        if target_prob[draft_token[i]] / draft_prob[draft_token[i]] >= uniform_random():
            ACCEPT draft_token[i]
        else:
            REJECT â†’ resample from normalized max(0, target_prob - draft_prob)
            BREAK (ì´í›„ ìœ„ì¹˜ëŠ” ëª¨ë‘ reject)

    if all k tokens accepted:
        sample BONUS token from target_prob[k]

Output: [batch, k+1] tensor, rejected positions = -1 (PLACEHOLDER)
```

### 6.2 êµ¬í˜„

```python
import mlx.core as mx


PLACEHOLDER_TOKEN_ID = -1


class BatchedRejectionSampler:
    """
    Batched speculative decoding rejection sampler.

    Source: vLLM V1 `vllm/v1/sample/rejection_sampler.py`
    í•µì‹¬ ì´ì‹ íŒ¨í„´: -1 paddingìœ¼ë¡œ ê°€ë³€ ê¸¸ì´ acceptance ì²˜ë¦¬.
    """

    def __call__(
        self,
        target_probs: mx.array,    # [batch, k+1, vocab]
        draft_probs: mx.array,     # [batch, k, vocab]
        draft_tokens: mx.array,    # [batch, k]
        proposal_lens: mx.array,   # [batch] â€” ì‹œí€€ìŠ¤ë³„ ìœ íš¨ proposal ê¸¸ì´
    ) -> mx.array:
        """
        Returns:
            output_tokens: [batch, k+1]
            accepted positions have token IDs, rejected = -1
        """
        return self._forward_vectorized(
            target_probs, draft_probs, draft_tokens, proposal_lens
        )

    def _forward_vectorized(
        self,
        target_probs: mx.array,
        draft_probs: mx.array,
        draft_tokens: mx.array,
        proposal_lens: mx.array,
    ) -> mx.array:
        """
        ë²¡í„°í™” ë²„ì „. ë°°ì¹˜ ì „ì²´ë¥¼ ë£¨í”„ ì—†ì´ í•œ ë²ˆì— ì²˜ë¦¬.
        """
        batch_size, k = draft_tokens.shape
        output = mx.full((batch_size, k + 1), PLACEHOLDER_TOKEN_ID, dtype=mx.int32)

        # 1) Draft tokenì— ëŒ€í•œ target/draft probability ì¶”ì¶œ
        # target_probs[:, :k, :] ì—ì„œ draft_tokens ìœ„ì¹˜ì˜ í™•ë¥ 
        idx = draft_tokens[:, :, None]  # [batch, k, 1]
        target_p = mx.take_along_axis(target_probs[:, :k, :], idx, axis=2).squeeze(-1)  # [batch, k]
        draft_p = mx.take_along_axis(draft_probs, idx, axis=2).squeeze(-1)              # [batch, k]

        # 2) Acceptance criterion: p_target / p_draft >= uniform
        rand = mx.random.uniform(shape=(batch_size, k))
        ratio = target_p / mx.maximum(draft_p, 1e-10)
        accepted = ratio >= rand  # [batch, k] bool

        # 3) proposal_lens mask: proposal ì—†ëŠ” ìœ„ì¹˜ëŠ” reject
        position_indices = mx.arange(k)[None, :]  # [1, k]
        valid_mask = position_indices < proposal_lens[:, None]  # [batch, k]
        accepted = accepted & valid_mask

        # 4) Left-to-right masking: ì²« rejection ì´í›„ëŠ” ëª¨ë‘ reject
        # cumprod trick: [T, T, F, T] â†’ [T, T, F, F]
        accepted_cumulative = mx.cumprod(accepted.astype(mx.float32), axis=1)
        accepted_mask = accepted_cumulative.astype(mx.bool_)  # [batch, k]

        # 5) Acceptëœ ìœ„ì¹˜ì— draft token ì±„ìš°ê¸°
        output[:, :k] = mx.where(accepted_mask, draft_tokens, PLACEHOLDER_TOKEN_ID)

        # 6) ì²« rejection ìœ„ì¹˜ì—ì„œ corrected distributionìœ¼ë¡œ resample
        num_accepted = accepted_mask.astype(mx.int32).sum(axis=1)  # [batch]
        for b in range(batch_size):
            n = int(num_accepted[b])
            plen = int(proposal_lens[b])
            if plen == 0:
                # proposal ì—†ì—ˆìŒ â†’ ì¼ë°˜ decode: targetì˜ top token
                output[b, 0] = mx.argmax(target_probs[b, 0, :])
            elif n < plen:
                # ì²« rejection ìœ„ì¹˜ì—ì„œ corrected distributionìœ¼ë¡œ resample
                corrected = mx.maximum(
                    target_probs[b, n, :] - draft_probs[b, n, :],
                    0.0
                )
                total = corrected.sum()
                if total > 1e-10:
                    corrected = corrected / total
                    output[b, n] = mx.random.categorical(mx.log(corrected + 1e-10))
                else:
                    output[b, n] = mx.argmax(target_probs[b, n, :])
            else:
                # ì „ë¶€ accept â†’ bonus token from target[k]
                output[b, k] = mx.random.categorical(
                    mx.log(target_probs[b, plen, :] + 1e-10)
                )

        return output

    def _forward_loop(
        self,
        target_probs: mx.array,
        draft_probs: mx.array,
        draft_tokens: mx.array,
        proposal_lens: mx.array,
    ) -> mx.array:
        """
        ë£¨í”„ ë²„ì „. ë””ë²„ê¹…/ê²€ì¦ìš©.
        ë²¡í„°í™” ë²„ì „ê³¼ ë™ì¼í•œ ê²°ê³¼ë¥¼ ë‚´ì•¼ í•¨.
        """
        batch_size, k = draft_tokens.shape
        output = mx.full((batch_size, k + 1), PLACEHOLDER_TOKEN_ID, dtype=mx.int32)

        for b in range(batch_size):
            plen = int(proposal_lens[b])
            if plen == 0:
                output[b, 0] = mx.argmax(target_probs[b, 0, :])
                continue

            accepted_count = 0
            for i in range(plen):
                token = int(draft_tokens[b, i])
                p_target = float(target_probs[b, i, token])
                p_draft = float(draft_probs[b, i, token])

                r = float(mx.random.uniform())
                if p_draft > 1e-10 and (p_target / p_draft) >= r:
                    output[b, i] = token
                    accepted_count += 1
                else:
                    corrected = mx.maximum(
                        target_probs[b, i, :] - draft_probs[b, i, :], 0.0
                    )
                    total = float(corrected.sum())
                    if total > 1e-10:
                        corrected = corrected / total
                        output[b, i] = mx.random.categorical(mx.log(corrected + 1e-10))
                    else:
                        output[b, i] = mx.argmax(target_probs[b, i, :])
                    break
            else:
                # All accepted â†’ bonus
                output[b, plen] = mx.random.categorical(
                    mx.log(target_probs[b, plen, :] + 1e-10)
                )

        return output
```

### 6.3 N-gram ì „ìš© Greedy Verifier

N-gramì€ draft probabilityê°€ ì—†ìœ¼ë¯€ë¡œ rejection sampling ëŒ€ì‹  greedy/threshold ê¸°ë°˜ verification ì‚¬ìš©.

```python
class NGramVerifier:
    """
    N-gram proposer ì „ìš© verifier.
    Draft probability ì—†ì´ target model outputë§Œìœ¼ë¡œ accept/reject ê²°ì •.
    """

    def __init__(self, mode: str = "greedy", threshold: float = 0.1):
        """
        Args:
            mode: "greedy" (target argmax == draft) ë˜ëŠ”
                  "threshold" (target_prob[draft] >= threshold)
            threshold: threshold ëª¨ë“œì—ì„œ acceptance ê¸°ì¤€ í™•ë¥ 
        """
        self.mode = mode
        self.threshold = threshold

    def __call__(
        self,
        target_probs: mx.array,   # [batch, k+1, vocab]
        draft_tokens: mx.array,   # [batch, k]
        proposal_lens: mx.array,  # [batch]
    ) -> mx.array:
        batch_size, k = draft_tokens.shape
        output = mx.full((batch_size, k + 1), PLACEHOLDER_TOKEN_ID, dtype=mx.int32)

        if self.mode == "greedy":
            return self._greedy(target_probs, draft_tokens, proposal_lens, output)
        else:
            return self._threshold(target_probs, draft_tokens, proposal_lens, output)

    def _greedy(self, target_probs, draft_tokens, proposal_lens, output):
        """Target modelì˜ argmaxì™€ draft tokenì´ ì¼ì¹˜í•˜ë©´ accept."""
        batch_size, k = draft_tokens.shape
        target_argmax = mx.argmax(target_probs[:, :k, :], axis=-1)  # [batch, k]

        match = (draft_tokens == target_argmax)  # [batch, k]

        # proposal_lens mask
        pos = mx.arange(k)[None, :]
        valid = pos < proposal_lens[:, None]
        match = match & valid

        # Left-to-right: ì²« ë¶ˆì¼ì¹˜ ì´í›„ ì „ë¶€ reject
        match_cum = mx.cumprod(match.astype(mx.float32), axis=1).astype(mx.bool_)

        output[:, :k] = mx.where(match_cum, draft_tokens, PLACEHOLDER_TOKEN_ID)

        # ì²« rejection ìœ„ì¹˜ì—ì„œ target argmaxë¡œ ëŒ€ì²´, ë˜ëŠ” bonus
        num_accepted = match_cum.astype(mx.int32).sum(axis=1)
        for b in range(batch_size):
            n = int(num_accepted[b])
            plen = int(proposal_lens[b])
            if plen == 0:
                output[b, 0] = mx.argmax(target_probs[b, 0, :])
            elif n < plen:
                output[b, n] = target_argmax[b, n]
            else:
                output[b, plen] = mx.argmax(target_probs[b, plen, :])

        return output

    def _threshold(self, target_probs, draft_tokens, proposal_lens, output):
        """Target modelì´ draft tokenì— ë¶€ì—¬í•œ í™•ë¥ ì´ threshold ì´ìƒì´ë©´ accept."""
        batch_size, k = draft_tokens.shape
        target_p = mx.take_along_axis(
            target_probs[:, :k, :], draft_tokens[:, :, None], axis=2
        ).squeeze(-1)  # [batch, k]

        accepted = target_p >= self.threshold

        pos = mx.arange(k)[None, :]
        valid = pos < proposal_lens[:, None]
        accepted = accepted & valid

        accepted_cum = mx.cumprod(accepted.astype(mx.float32), axis=1).astype(mx.bool_)
        output[:, :k] = mx.where(accepted_cum, draft_tokens, PLACEHOLDER_TOKEN_ID)

        num_accepted = accepted_cum.astype(mx.int32).sum(axis=1)
        for b in range(batch_size):
            n = int(num_accepted[b])
            plen = int(proposal_lens[b])
            if plen == 0:
                output[b, 0] = mx.argmax(target_probs[b, 0, :])
            elif n < plen:
                output[b, n] = mx.argmax(target_probs[b, n, :])
            else:
                output[b, plen] = mx.argmax(target_probs[b, plen, :])

        return output
```

---

## 7. Phase 2: Draft Model Proposer (spec_decode/proposer/draft_model.py)

### 7.1 ì„¤ê³„ ê·¼ê±°

- mlx-lmì˜ `speculative_generate_step`ì—ì„œ single-stream ë¡œì§ ì¶”ì¶œ
- ì´ë¥¼ batch ëª¨ë“œë¡œ í™•ì¥: ëª¨ë“  ì‹œí€€ìŠ¤ì— ëŒ€í•´ draft modelì„ ë™ì‹œì— ì‹¤í–‰
- MLX lazy evaluationìœ¼ë¡œ k stepì˜ draftë¥¼ í•˜ë‚˜ì˜ ê³„ì‚° ê·¸ë˜í”„ë¡œ fusion
- Draft modelì€ target modelê³¼ ë³„ê°œë¡œ ë¡œë”© (mlx-lmì˜ `load` í•¨ìˆ˜ ì‚¬ìš©)

### 7.2 êµ¬í˜„

```python
from typing import List, Optional

import mlx.core as mx
import mlx.nn as nn
from mlx_lm import load as mlx_load

from .base import BaseProposer, ProposalResult


class DraftModelProposer(BaseProposer):
    """
    ì†Œí˜• draft modelë¡œ kê°œ í† í°ì„ batch ìƒì„±.

    mlx-lmì˜ speculative_generate_step (batch=1)ì„ batch ëª¨ë“œë¡œ í™•ì¥.
    Draft modelê³¼ target modelì€ ë³„ë„ KV cacheë¥¼ ìœ ì§€.

    MLX lazy evaluation í™œìš©:
    - k stepì˜ draftë¥¼ mx.eval() ì—†ì´ ì—°ì† ì‹¤í–‰
    - ë§ˆì§€ë§‰ì— í•œ ë²ˆì— evaluate â†’ ìë™ operation fusion
    """

    def __init__(self, model_path: str, quantize: Optional[str] = None):
        self.model_path = model_path
        self.quantize = quantize
        self.model: Optional[nn.Module] = None
        self.tokenizer = None
        self._loaded = False

    def load(self):
        """Draft model ë¡œë”©. ì—”ì§„ ì´ˆê¸°í™” ì‹œ 1íšŒ í˜¸ì¶œ."""
        if self._loaded:
            return
        # mlx-lmì˜ load í•¨ìˆ˜ë¡œ ëª¨ë¸ + í† í¬ë‚˜ì´ì € ë¡œë”©
        self.model, self.tokenizer = mlx_load(self.model_path)
        self._loaded = True

    @property
    def needs_draft_probs(self) -> bool:
        return True  # rejection samplingì— draft probability í•„ìš”

    @property
    def requires_gpu(self) -> bool:
        return True  # Metal GPU ì‚¬ìš©

    def propose(
        self,
        sequences: List["Sequence"],
        k: int,
    ) -> Optional[ProposalResult]:
        if not self._loaded:
            self.load()

        batch_size = len(sequences)
        all_draft_tokens: List[mx.array] = []
        all_draft_probs: List[mx.array] = []

        # â”€â”€â”€ k step autoregressive draft generation â”€â”€â”€
        for step in range(k):
            # ë°°ì¹˜ ì…ë ¥ ì¤€ë¹„: ê° ì‹œí€€ìŠ¤ì˜ í˜„ì¬ ë§ˆì§€ë§‰ í† í°
            if step == 0:
                input_ids = mx.array(
                    [seq.last_token_id for seq in sequences],
                    dtype=mx.int32,
                )[:, None]  # [batch, 1]
            else:
                input_ids = all_draft_tokens[-1][:, None]  # [batch, 1]

            # Draft model forward (ë°°ì¹˜)
            # â˜… ê¸°ì¡´ continuous batchingì˜ forwardì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤
            #    ë‹¨, draft modelì˜ KV cache ì‚¬ìš©
            logits = self._batched_forward(sequences, input_ids, step)
            # logits: [batch, 1, vocab] â†’ squeeze
            logits = logits[:, -1, :]  # [batch, vocab]

            probs = mx.softmax(logits, axis=-1)    # [batch, vocab]
            tokens = mx.random.categorical(mx.log(probs + 1e-10))  # [batch]

            all_draft_tokens.append(tokens)
            all_draft_probs.append(probs)

            # âš ï¸ mx.eval() í˜¸ì¶œí•˜ì§€ ì•ŠìŒ â†’ lazy evaluationìœ¼ë¡œ fusion

        # â”€â”€â”€ í•œ ë²ˆì— evaluate â”€â”€â”€
        draft_tokens = mx.stack(all_draft_tokens, axis=1)  # [batch, k]
        draft_probs = mx.stack(all_draft_probs, axis=1)    # [batch, k, vocab]
        mx.eval(draft_tokens, draft_probs)

        proposal_lens = mx.full((batch_size,), k, dtype=mx.int32)

        return ProposalResult(
            draft_tokens=draft_tokens,
            draft_probs=draft_probs,
            proposal_lens=proposal_lens,
        )

    def _batched_forward(
        self,
        sequences: List["Sequence"],
        input_ids: mx.array,
        step: int,
    ) -> mx.array:
        """
        Draft modelì˜ batched forward pass.
        ê° ì‹œí€€ìŠ¤ë³„ draft KV cacheë¥¼ ìœ ì§€í•˜ë©´ì„œ forward.

        â˜… êµ¬í˜„ ì‹œ ê¸°ì¡´ ì—”ì§„ì˜ batched forward ì¸í„°í˜ì´ìŠ¤ì— ë§ì¶° ì¡°ì •.
           draft model ì „ìš© KV cacheë¥¼ ë³„ë„ë¡œ ê´€ë¦¬í•´ì•¼ í•¨.
        """
        # TODO: draft model KV cache management + batched forward
        # í•µì‹¬: step=0ì´ë©´ target modelì˜ KV cache ìƒíƒœì—ì„œ ì‹œì‘
        #        step>0ì´ë©´ ì´ì „ draft stepì˜ KVì— ì´ì–´ì„œ ì§„í–‰
        raise NotImplementedError("Draft model batched forward")

    def reset_draft_cache(self, request_ids: List[str]):
        """ë§¤ engine step ì‹œì‘ ì‹œ draft KV cache ë¦¬ì…‹."""
        # Draft cacheëŠ” ì¼ì‹œì  â†’ ë§¤ step ìƒˆë¡œ ìƒì„±
        pass
```

### 7.3 Draft Model ë©”ëª¨ë¦¬ ê´€ë¦¬

```
Draft model ë©”ëª¨ë¦¬ ë ˆì´ì•„ì›ƒ:
- Target model: ë©”ì¸ KV cache (persistent, ìš”ì²­ ìˆ˜ëª… ë™ì•ˆ ìœ ì§€)
- Draft model: ì„ì‹œ KV cache (ë§¤ engine stepë§ˆë‹¤ ë¦¬ì…‹)

Step íë¦„:
1. Draft model KV cacheë¥¼ target modelì˜ í˜„ì¬ ìƒíƒœì—ì„œ fork
   - Apple Silicon: unified memory â†’ view/shallow copy ê°€ëŠ¥
   - CUDAì—ì„œëŠ” deep copy í•„ìš”í•˜ì§€ë§Œ MLXì—ì„œëŠ” ë¶ˆí•„ìš”
2. Draft modelì´ k step ë™ì•ˆ ìì²´ KV cache extend
3. Verification ì™„ë£Œ í›„ draft KV cache íê¸°
4. Target model KV cacheëŠ” accepted token ìˆ˜ë§Œí¼ë§Œ extend
```

---

## 8. Phase 3: EAGLE Proposer (spec_decode/proposer/eagle.py)

### 8.1 ì„¤ê³„ ê·¼ê±°

- EAGLE: target modelì˜ hidden statesë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ multi-token prediction
- ë³„ë„ ëª¨ë¸ì´ ì•„ë‹Œ ì¶”ê°€ prediction headë§Œ í•„ìš” â†’ ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- vLLM V1 roadmapì—ì„œ ìš°ì„  ì§€ì› ì˜ˆì •
- Draft model ëŒ€ë¹„ ì¥ì : vocab projection ì´ë¯¸ ì™„ë£Œëœ hidden states í™œìš© â†’ ì •í™•ë„ ë†’ìŒ

### 8.2 êµ¬í˜„

```python
from typing import List, Optional

import mlx.core as mx
import mlx.nn as nn

from .base import BaseProposer, ProposalResult


class EAGLEHead(nn.Module):
    """
    EAGLE prediction head.
    Target modelì˜ hidden statesë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„
    ë‹¤ìŒ ì—¬ëŸ¬ í† í°ì˜ í™•ë¥  ë¶„í¬ë¥¼ ì˜ˆì¸¡.

    êµ¬ì¡°: target hidden state â†’ FC layers â†’ vocab projection
    """

    def __init__(
        self,
        hidden_size: int,
        vocab_size: int,
        num_layers: int = 1,
        intermediate_size: Optional[int] = None,
    ):
        super().__init__()
        intermediate_size = intermediate_size or hidden_size

        layers = []
        for i in range(num_layers):
            in_size = hidden_size if i == 0 else intermediate_size
            out_size = intermediate_size if i < num_layers - 1 else hidden_size
            layers.append(nn.Linear(in_size, out_size))
            if i < num_layers - 1:
                layers.append(nn.SiLU())

        self.fc = nn.Sequential(*layers)
        self.lm_head = nn.Linear(hidden_size, vocab_size, bias=False)

    def __call__(self, hidden_states: mx.array) -> mx.array:
        """
        Args:
            hidden_states: [batch, hidden_size] â€” target modelì˜ ë§ˆì§€ë§‰ hidden state

        Returns:
            logits: [batch, vocab_size]
        """
        h = self.fc(hidden_states)
        return self.lm_head(h)


class EAGLEProposer(BaseProposer):
    """
    EAGLE-style speculative decoding.

    Target modelì˜ hidden states + ì¶”ê°€ prediction headë¡œ draft token ìƒì„±.
    ë³„ë„ ëª¨ë¸ ë¶ˆí•„ìš” â†’ prediction headë§Œ ë¡œë”©.

    autoregressiveí•˜ê²Œ k step ì‹¤í–‰:
    1. targetì˜ last hidden state â†’ EAGLE head â†’ í† í° 1 ì˜ˆì¸¡
    2. ì˜ˆì¸¡ëœ í† í°ì„ target modelì— ë„£ì–´ ë‹¤ìŒ hidden state íšë“
    3. ê·¸ hidden stateë¡œ EAGLE head â†’ í† í° 2 ì˜ˆì¸¡
    4. ... kíšŒ ë°˜ë³µ

    â˜… ì£¼ì˜: step 2ì—ì„œ target modelì˜ forwardê°€ í•„ìš”í•˜ë¯€ë¡œ
       draft modelë³´ë‹¤ computationì´ í¬ì§€ë§Œ, ì •í™•ë„ê°€ í›¨ì”¬ ë†’ìŒ.
       "self-speculation" ë°©ì‹.

    ëŒ€ì•ˆ (MTP): target model ìì²´ì— multi-token prediction headê°€ ìˆëŠ” ê²½ìš°
    (DeepSeek V3 ë“±), ë³„ë„ EAGLE head ì—†ì´ ëª¨ë¸ ë‚´ì¥ MTP head í™œìš©.
    """

    def __init__(
        self,
        target_model: nn.Module,
        head_path: Optional[str] = None,
        num_layers: int = 1,
    ):
        self.target_model = target_model
        self.head_path = head_path
        self.num_layers = num_layers
        self.eagle_head: Optional[EAGLEHead] = None
        self._loaded = False

    def load(self):
        """EAGLE head ë¡œë”©. ì—†ìœ¼ë©´ ìƒˆë¡œ ì´ˆê¸°í™” (í•™ìŠµ í•„ìš”)."""
        if self._loaded:
            return

        # Target modelì—ì„œ hidden_size, vocab_size ì¶”ì¶œ
        # â˜… ëª¨ë¸ ì•„í‚¤í…ì²˜ì— ë”°ë¼ ì ‘ê·¼ ë°©ì‹ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
        config = self.target_model.config if hasattr(self.target_model, 'config') else None
        hidden_size = getattr(config, 'hidden_size', 4096)
        vocab_size = getattr(config, 'vocab_size', 128256)

        self.eagle_head = EAGLEHead(
            hidden_size=hidden_size,
            vocab_size=vocab_size,
            num_layers=self.num_layers,
        )

        if self.head_path:
            # ì‚¬ì „ í•™ìŠµëœ EAGLE head ê°€ì¤‘ì¹˜ ë¡œë”©
            weights = mx.load(self.head_path)
            self.eagle_head.load_weights(list(weights.items()))

        self._loaded = True

    @property
    def needs_draft_probs(self) -> bool:
        return True

    @property
    def requires_gpu(self) -> bool:
        return True

    def propose(
        self,
        sequences: List["Sequence"],
        k: int,
    ) -> Optional[ProposalResult]:
        if not self._loaded:
            self.load()

        batch_size = len(sequences)
        all_tokens: List[mx.array] = []
        all_probs: List[mx.array] = []

        # ì´ˆê¸° hidden state: target modelì˜ ë§ˆì§€ë§‰ hidden state
        # â˜… ê¸°ì¡´ ì—”ì§„ì—ì„œ forward ì‹œ hidden stateë¥¼ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì • í•„ìš”
        hidden = self._get_last_hidden_states(sequences)  # [batch, hidden_size]

        for step in range(k):
            logits = self.eagle_head(hidden)  # [batch, vocab]
            probs = mx.softmax(logits, axis=-1)
            tokens = mx.random.categorical(mx.log(probs + 1e-10))

            all_tokens.append(tokens)
            all_probs.append(probs)

            # ë‹¤ìŒ stepì„ ìœ„í•œ hidden state ì—…ë°ì´íŠ¸
            # ì˜ˆì¸¡ëœ í† í°ì„ target modelì— ë„£ì–´ hidden state íšë“
            hidden = self._get_hidden_for_token(sequences, tokens)

        draft_tokens = mx.stack(all_tokens, axis=1)
        draft_probs = mx.stack(all_probs, axis=1)
        mx.eval(draft_tokens, draft_probs)

        return ProposalResult(
            draft_tokens=draft_tokens,
            draft_probs=draft_probs,
            proposal_lens=mx.full((batch_size,), k, dtype=mx.int32),
        )

    def _get_last_hidden_states(self, sequences) -> mx.array:
        """
        ê° ì‹œí€€ìŠ¤ì— ëŒ€í•œ target modelì˜ ë§ˆì§€ë§‰ hidden state ì¶”ì¶œ.
        â˜… ê¸°ì¡´ ì—”ì§„ ìˆ˜ì • í•„ìš”: forward ì‹œ hidden state ìºì‹±.
        """
        raise NotImplementedError(
            "Target model forwardì—ì„œ hidden state ë°˜í™˜ ì¸í„°í˜ì´ìŠ¤ í•„ìš”"
        )

    def _get_hidden_for_token(self, sequences, tokens) -> mx.array:
        """
        ì˜ˆì¸¡ëœ í† í°ì„ target model embedding + ì¼ë¶€ layerì— í†µê³¼ì‹œì¼œ
        hidden state íšë“. Full forward ëŒ€ë¹„ lightweight.
        """
        raise NotImplementedError(
            "Target modelì˜ lightweight hidden state ê³„ì‚°"
        )
```

### 8.3 EAGLE Training (ë³„ë„ í”„ë¡œì„¸ìŠ¤)

```python
# EAGLE head í•™ìŠµì€ servingê³¼ ë³„ë„ í”„ë¡œì„¸ìŠ¤
# target modelì˜ (hidden_state, next_token) ìŒì„ ë°ì´í„°ë¡œ í•™ìŠµ

def train_eagle_head(
    target_model_path: str,
    train_data_path: str,
    output_path: str,
    num_layers: int = 1,
    epochs: int = 3,
    lr: float = 1e-4,
):
    """
    EAGLE head í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸.

    ë°ì´í„°: target modelì„ ì‹¤í–‰í•˜ë©´ì„œ ìˆ˜ì§‘í•œ (hidden_state, next_token) ìŒ
    ì†ì‹¤: cross-entropy loss

    ì‚¬ìš©:
    python -m hwquant.spec_decode.train_eagle \\
        --target-model mlx-community/Qwen3-32B-4bit \\
        --train-data /path/to/data \\
        --output /path/to/eagle_head.safetensors
    """
    pass  # TODO: í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ êµ¬í˜„
```

---

## 9. Phase 4: Dynamic Controller (spec_decode/dynamic_controller.py)

### 9.1 ì„¤ê³„ ê·¼ê±°

- vLLM ë²¤ì¹˜ë§ˆí¬: spec decodeëŠ” low QPSì—ì„œ 1.5â€“2.8x speedup, high QPSì—ì„œ 1.4â€“1.8x slowdown
- `disable_by_batch_size`: batch size threshold ì´ˆê³¼ ì‹œ ìë™ OFF
- Acceptance rate EMA: ë‚®ìœ¼ë©´ speculation ê°•ë„ ì¤„ì´ê±°ë‚˜ OFF
- Per-request granularity: request-level override ì§€ì›

### 9.2 êµ¬í˜„

```python
from dataclasses import dataclass, field
import time
from typing import Dict, Optional

from .config import SpecDecodeConfig


@dataclass
class SpecDecodeStats:
    """Stepë³„ í†µê³„."""
    total_proposed: int = 0
    total_accepted: int = 0
    total_steps: int = 0
    total_bonus_tokens: int = 0

    @property
    def acceptance_rate(self) -> float:
        if self.total_proposed == 0:
            return 0.0
        return self.total_accepted / self.total_proposed

    @property
    def avg_tokens_per_step(self) -> float:
        """Stepë‹¹ í‰ê·  ìƒì„± í† í° ìˆ˜. 1.0 = spec decode íš¨ê³¼ ì—†ìŒ."""
        if self.total_steps == 0:
            return 1.0
        return (self.total_accepted + self.total_bonus_tokens) / self.total_steps


class DynamicSpecController:
    """
    ë°°ì¹˜ í¬ê¸°ì™€ acceptance rateì— ë”°ë¼ speculation ê°•ë„ë¥¼ ë™ì  ì¡°ì ˆ.

    ê²°ì • íë¦„:
    1. batch_size >= disable_threshold â†’ spec OFF
    2. acceptance_rate_ema < acceptance_rate_threshold â†’ spec OFF
    3. acceptance_rate_emaì— ë”°ë¼ k ì¡°ì ˆ:
       - > 0.8 â†’ k = max (ê³µê²©ì )
       - 0.5â€“0.8 â†’ k = max - 2 (ë³´ìˆ˜ì )
       - 0.3â€“0.5 â†’ k = 1 (ìµœì†Œ)
       - < 0.3 â†’ OFF
    """

    def __init__(self, config: SpecDecodeConfig):
        self.config = config
        self.acceptance_rate_ema: float = 0.7  # ì´ˆê¸°ê°’ (optimistic)
        self.stats = SpecDecodeStats()

        # ìµœê·¼ N stepì˜ ìƒì„¸ ê¸°ë¡ (ë””ë²„ê¹… ë° ëª¨ë‹ˆí„°ë§)
        self._recent_rates: list[float] = []
        self._max_recent = 100

    def should_speculate(self, batch_size: int) -> bool:
        """í˜„ì¬ ì¡°ê±´ì—ì„œ spec decodeë¥¼ í™œì„±í™”í• ì§€ ê²°ì •."""
        if self.config.spec_decode_mode == "none":
            return False

        # Batch size threshold
        if batch_size >= self.config.disable_by_batch_size:
            return False

        # Dynamic controlì´ êº¼ì ¸ ìˆìœ¼ë©´ í•­ìƒ ON
        if not self.config.dynamic_spec_decode:
            return True

        # Acceptance rate threshold
        return self.acceptance_rate_ema >= self.config.acceptance_rate_threshold

    def get_num_spec_tokens(self, batch_size: int) -> int:
        """ë°°ì¹˜ í¬ê¸°ì™€ acceptance rateì— ë”°ë¼ k ê²°ì •."""
        if not self.should_speculate(batch_size):
            return 0

        k = self.config.num_speculative_tokens

        if not self.config.adaptive_k:
            return k

        # Adaptive k: acceptance rateì— ë”°ë¼ ì¡°ì ˆ
        if self.acceptance_rate_ema > 0.8:
            return k                          # ìµœëŒ€ speculation
        elif self.acceptance_rate_ema > 0.5:
            return max(1, k - 2)              # ë³´ìˆ˜ì 
        elif self.acceptance_rate_ema > 0.3:
            return 1                          # ìµœì†Œ
        else:
            return 0                          # OFF

    def update(self, num_proposed: int, num_accepted: int, num_bonus: int = 0):
        """
        ë§¤ engine step í›„ í˜¸ì¶œ.

        Args:
            num_proposed: ë°°ì¹˜ ì „ì²´ì—ì„œ ì œì•ˆëœ ì´ draft í† í° ìˆ˜
            num_accepted: ë°°ì¹˜ ì „ì²´ì—ì„œ acceptëœ ì´ í† í° ìˆ˜
            num_bonus: ë°°ì¹˜ ì „ì²´ì—ì„œ ìƒì„±ëœ bonus í† í° ìˆ˜
        """
        self.stats.total_proposed += num_proposed
        self.stats.total_accepted += num_accepted
        self.stats.total_bonus_tokens += num_bonus
        self.stats.total_steps += 1

        if num_proposed > 0:
            step_rate = num_accepted / num_proposed
            alpha = self.config.acceptance_rate_ema_alpha
            self.acceptance_rate_ema = alpha * step_rate + (1 - alpha) * self.acceptance_rate_ema

            self._recent_rates.append(step_rate)
            if len(self._recent_rates) > self._max_recent:
                self._recent_rates.pop(0)

    def get_metrics(self) -> Dict:
        """ëª¨ë‹ˆí„°ë§ìš© ë©”íŠ¸ë¦­ ë°˜í™˜. /metrics endpoint ë“±ì—ì„œ ì‚¬ìš©."""
        return {
            "spec_decode_enabled": self.config.spec_decode_mode != "none",
            "spec_decode_mode": self.config.spec_decode_mode,
            "acceptance_rate_ema": round(self.acceptance_rate_ema, 4),
            "acceptance_rate_overall": round(self.stats.acceptance_rate, 4),
            "avg_tokens_per_step": round(self.stats.avg_tokens_per_step, 2),
            "total_steps": self.stats.total_steps,
            "total_proposed": self.stats.total_proposed,
            "total_accepted": self.stats.total_accepted,
            "total_bonus_tokens": self.stats.total_bonus_tokens,
            "current_k": self.config.num_speculative_tokens,
        }
```

---

## 10. KV Cache Manager (spec_decode/kv_manager.py)

```python
from typing import Dict, List

import mlx.core as mx


class SpecDecodeKVManager:
    """
    Spec decode ì „ìš© KV cache ê´€ë¦¬.
    ê¸°ì¡´ continuous batching ì—”ì§„ì˜ KV managerë¥¼ í™•ì¥.

    Apple Silicon í•µì‹¬ ì´ì :
    - Unified memory â†’ zero-copy, KV ì „ì†¡ ë¶ˆí•„ìš”
    - Rollback = cache length ì¡°ì • (ê±°ì˜ ë¬´ë¹„ìš©)
    - vLLMì˜ PagedAttention block deallocation ë¶ˆí•„ìš”
    """

    def __init__(self, base_kv_manager):
        """
        Args:
            base_kv_manager: ê¸°ì¡´ continuous batching ì—”ì§„ì˜ KV manager ì°¸ì¡°
        """
        self.base = base_kv_manager

    def pre_allocate(self, request_id: str, num_tokens: int):
        """
        Verificationì„ ìœ„í•´ KV cache slots ì‚¬ì „ í• ë‹¹.
        MLX lazy allocation â†’ ì‹¤ì œ forward ì‹¤í–‰ ì „ê¹Œì§€ ë©”ëª¨ë¦¬ ì ìœ  ì—†ìŒ.
        """
        self.base.allocate_slots(request_id, num_tokens)

    def rollback(self, request_id: str, accepted_count: int, total_proposed: int):
        """
        Rejection ë°œìƒ ì‹œ KV cache rollback.

        CUDA (vLLM): block deallocation (PagedAttention)
        MLX: cache sequence length ì¡°ì •ë§Œ â†’ near zero-cost

        Args:
            request_id: ìš”ì²­ ID
            accepted_count: acceptëœ í† í° ìˆ˜
            total_proposed: ì „ì²´ ì œì•ˆëœ í† í° ìˆ˜
        """
        rejected_count = total_proposed - accepted_count
        if rejected_count > 0:
            # ê¸°ì¡´ KV managerì— truncate ìš”ì²­
            self.base.truncate(request_id, num_tokens_to_remove=rejected_count)

    def commit(self, request_id: str, num_accepted: int):
        """
        Acceptëœ í† í°ì˜ KVë¥¼ ì˜êµ¬ í™•ì •.
        Target model verificationì—ì„œ ì´ë¯¸ ê³„ì‚°ëœ KVë¥¼ ìœ ì§€.
        """
        self.base.confirm_extension(request_id, num_accepted)

    def reset_draft_cache(self, request_ids: List[str]):
        """
        Draft model KV cache ë¦¬ì…‹ (Phase 2ìš©).
        ë§¤ engine step ì‹œì‘ ì‹œ í˜¸ì¶œ.
        """
        # Draft cacheëŠ” ì¼ì‹œì  â†’ stepë§ˆë‹¤ ìƒˆë¡œ ìƒì„±
        for rid in request_ids:
            if hasattr(self.base, 'reset_draft'):
                self.base.reset_draft(rid)
```

---

## 11. Engine í†µí•© (engine/engine.py ìˆ˜ì •)

ê¸°ì¡´ continuous batching ì—”ì§„ì˜ `step()` ë©”ì„œë“œì— spec decodeë¥¼ ì‚½ì….

```python
# engine/engine.py ìˆ˜ì • ì‚¬í•­

from spec_decode.config import SpecDecodeConfig
from spec_decode.proposer.base import create_proposer, ProposalResult
from spec_decode.verifier import BatchedVerifier
from spec_decode.rejection_sampler import (
    BatchedRejectionSampler, NGramVerifier, PLACEHOLDER_TOKEN_ID
)
from spec_decode.dynamic_controller import DynamicSpecController
from spec_decode.kv_manager import SpecDecodeKVManager


class ServingEngine:
    def __init__(self, model, tokenizer, config, spec_config: SpecDecodeConfig):
        # ... ê¸°ì¡´ ì´ˆê¸°í™” ...

        # â”€â”€â”€ Spec decode ì´ˆê¸°í™” â”€â”€â”€
        self.spec_config = spec_config
        self.proposer = create_proposer(spec_config, target_model=model)
        self.verifier = BatchedVerifier(model, tokenizer) if self.proposer else None
        self.rejection_sampler = BatchedRejectionSampler()
        self.ngram_verifier = NGramVerifier(mode="greedy")
        self.dynamic_controller = DynamicSpecController(spec_config)
        self.spec_kv_manager = SpecDecodeKVManager(self.kv_manager)

        # Draft model ë¡œë”© (Phase 2)
        if self.proposer and hasattr(self.proposer, 'load'):
            self.proposer.load()

    async def step(self):
        """
        ë©”ì¸ ì—”ì§„ ë£¨í”„ 1 step.
        Spec decodeê°€ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ propose â†’ verify â†’ accept/reject íë¦„.
        ë¹„í™œì„±í™”ë©´ ê¸°ì¡´ continuous batching ë¡œì§ ê·¸ëŒ€ë¡œ.
        """
        # â•â•â• Phase 0: Schedule â•â•â•
        scheduled_requests = self.scheduler.schedule()
        if not scheduled_requests:
            return

        prefill_reqs = [r for r in scheduled_requests if r.state == "PREFILL"]
        decode_reqs = [r for r in scheduled_requests if r.state == "DECODE"]

        # Prefill ì²˜ë¦¬ (spec decode ì ìš© ì•ˆ í•¨)
        if prefill_reqs:
            await self._batched_prefill(prefill_reqs)
            # Prefill ì™„ë£Œëœ ì‹œí€€ìŠ¤ì˜ n-gram index ì´ˆê¸°í™” (Phase 1)
            if self.spec_config.spec_decode_mode == "ngram":
                for req in prefill_reqs:
                    self._init_ngram_index(req)

        if not decode_reqs:
            return

        # Spec decode í™œì„±í™” ì—¬ë¶€ ê²°ì •
        batch_size = len(decode_reqs)
        use_spec = (
            self.proposer is not None
            and self.dynamic_controller.should_speculate(batch_size)
        )

        if not use_spec:
            # ê¸°ì¡´ continuous batching ë¡œì§
            await self._normal_batched_decode(decode_reqs)
            return

        # â•â•â• Phase 1: Propose â•â•â•
        k = self.dynamic_controller.get_num_spec_tokens(batch_size)
        if k == 0:
            await self._normal_batched_decode(decode_reqs)
            return

        proposal = self.proposer.propose(decode_reqs, k)
        if proposal is None:
            # ì „ì²´ ë°°ì¹˜ proposal ì‹¤íŒ¨ â†’ ì¼ë°˜ decode
            await self._normal_batched_decode(decode_reqs)
            return

        # KV cache pre-allocation
        for i, req in enumerate(decode_reqs):
            plen = int(proposal.proposal_lens[i])
            if plen > 0:
                self.spec_kv_manager.pre_allocate(req.id, plen + 1)

        # â•â•â• Phase 2: Verify â•â•â•
        target_probs = self.verifier.verify(
            decode_reqs, proposal.draft_tokens, proposal.proposal_lens
        )

        # â•â•â• Phase 3: Accept / Reject â•â•â•
        if self.proposer.needs_draft_probs:
            # Draft model / EAGLE â†’ rejection sampling
            accepted_tokens = self.rejection_sampler(
                target_probs,
                proposal.draft_probs,
                proposal.draft_tokens,
                proposal.proposal_lens,
            )
        else:
            # N-gram â†’ greedy verification
            accepted_tokens = self.ngram_verifier(
                target_probs,
                proposal.draft_tokens,
                proposal.proposal_lens,
            )

        # â•â•â• Phase 4: Postprocess â•â•â•
        total_proposed = 0
        total_accepted = 0
        total_bonus = 0

        for i, req in enumerate(decode_reqs):
            tokens_row = accepted_tokens[i]  # [k+1]
            plen = int(proposal.proposal_lens[i])

            # -1 ì œê±°í•˜ì—¬ ìœ íš¨ í† í°ë§Œ ì¶”ì¶œ
            valid_mask = tokens_row != PLACEHOLDER_TOKEN_ID
            valid_tokens = tokens_row[valid_mask]
            n_valid = int(valid_tokens.shape[0])

            # ì‹œí€€ìŠ¤ì— í† í° ì¶”ê°€
            req.append_tokens(valid_tokens)

            # KV cache ì •ë¦¬
            if plen > 0:
                if n_valid <= plen:
                    # ì¼ë¶€ reject â†’ rollback
                    self.spec_kv_manager.rollback(req.id, n_valid, plen + 1)
                else:
                    # ì „ë¶€ accept + bonus
                    self.spec_kv_manager.commit(req.id, n_valid)
                    total_bonus += 1

            total_proposed += plen
            total_accepted += min(n_valid, plen)

            # N-gram index incremental update
            if self.spec_config.spec_decode_mode == "ngram" and n_valid > 0:
                self._update_ngram_index(req, n_valid)

            # ì™„ë£Œ ì²´í¬
            if req.is_finished():
                self.scheduler.finish(req)
                await self._yield_output(req)

        # í†µê³„ ì—…ë°ì´íŠ¸
        self.dynamic_controller.update(total_proposed, total_accepted, total_bonus)

        # Draft model cache ë¦¬ì…‹ (Phase 2)
        if self.proposer and self.proposer.requires_gpu:
            self.spec_kv_manager.reset_draft_cache([r.id for r in decode_reqs])

    def _init_ngram_index(self, req):
        """Prefill ì™„ë£Œ í›„ n-gram suffix index ì´ˆê¸°í™”."""
        from spec_decode.proposer.ngram import NGramProposer
        context = req.prompt_tokens + req.generated_tokens
        req._ngram_index = NGramProposer.build_suffix_index(
            context, self.spec_config.ngram_max
        )
        req._ngram_dirty = False

    def _update_ngram_index(self, req, new_token_count):
        """ìƒˆ í† í° ì¶”ê°€ ì‹œ n-gram index incremental ì—…ë°ì´íŠ¸."""
        from spec_decode.proposer.ngram import NGramProposer
        context = req.prompt_tokens + req.generated_tokens
        NGramProposer.update_suffix_index(
            req._ngram_index, context, new_token_count, self.spec_config.ngram_max
        )
```

---

## 12. API Endpoint ìˆ˜ì • (serve.py)

```python
# /metrics endpointì— spec decode í†µê³„ ì¶”ê°€
@app.get("/v1/spec_decode/metrics")
async def spec_decode_metrics():
    return engine.dynamic_controller.get_metrics()

# /v1/chat/completionsì—ì„œ extra_body override ì²˜ë¦¬
async def handle_chat_completion(request: ChatCompletionRequest):
    # request-level spec decode override
    spec_override = getattr(request, 'extra_body', {})
    if 'spec_decode' in spec_override:
        # ì´ requestì— ëŒ€í•´ì„œë§Œ spec decode ëª¨ë“œ ë³€ê²½
        # â˜… per-request configëŠ” ë³„ë„ êµ¬í˜„ í•„ìš”
        pass
```

---

## 13. Source Code References

ì´ì‹ ëŒ€ìƒ vLLM/mlx-lm ì†ŒìŠ¤ì½”ë“œ:

| Component | Source File | ì´ì‹ í•µì‹¬ |
|-----------|-----------|----------|
| V0 Spec Worker | `vllm/spec_decode/spec_decode_worker.py` | `disable_by_batch_size`, proposer/scorer íŒ¨í„´ |
| V0 Batch Expansion | `vllm/spec_decode/batch_expansion.py` | ì‹œí€€ìŠ¤ ë³µì œ verification (Apple Siliconì—ì„œëŠ” ë¶ˆí•„ìš”) |
| V1 Rejection Sampler | `vllm/v1/sample/rejection_sampler.py` | -1 padding, bonus token, ë²¡í„°í™” |
| V1 Scheduler | `vllm/v1/core/sched/scheduler.py` | `{req_id: num_tokens}` í†µí•© ì˜ˆì‚° |
| V1 Model Runner | `vllm/v1/worker/gpu_model_runner.py` | `_calc_spec_decode_metadata()` |
| NGram Worker | `vllm/spec_decode/ngram_worker.py` | Draft-free n-gram speculation ì•Œê³ ë¦¬ì¦˜ |
| Top1 Proposer | `vllm/spec_decode/top1_proposer.py` | Non-spec sequence ì²˜ë¦¬, proposal ê¸¸ì´ 0 í•¸ë“¤ë§ |
| mlx-lm Spec Gen | `mlx_lm/generate.py` â†’ `speculative_generate_step` | ë‹¨ì¼ ì‹œí€€ìŠ¤ spec decode ë¡œì§ |
| mlx-lm Batch Gen | `mlx_lm/generate.py` | ë°°ì¹˜ ìƒì„± ì¸í”„ë¼ |
| vllm-mlx Paper | `arxiv.org/html/2601.19139v2` | Continuous batching ì•„í‚¤í…ì²˜ |

---

## 14. CUDA vs Apple Silicon ì°¨ì´ ì •ë¦¬

êµ¬í˜„ ì‹œ vLLM ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì´ì‹í•˜ë©´ ì•ˆ ë˜ëŠ” ë¶€ë¶„:

| vLLM (CUDA) | Apple Silicon (MLX) ëŒ€ì‘ |
|-------------|-------------------------|
| PagedAttention block alloc/dealloc | ì—°ì† ë©”ëª¨ë¦¬ + length ê¸°ë°˜ ê´€ë¦¬ |
| GPUâ†”CPU explicit KV copy | Zero-copy (unified memory) |
| Batch expansion (tensor ë³µì œ) | Padding + mask (ë³µì œ ë¶ˆí•„ìš”) |
| CUDA graphs (draft step fusion) | MLX lazy eval â†’ ìë™ fusion |
| Block table management | ë¶ˆí•„ìš” â†’ OS-level paging |
| `torch.where` / `torch.scatter` | `mx.where` / `mx.take_along_axis` |
| `torch.multinomial` | `mx.random.categorical` |

---

## 15. êµ¬í˜„ ìˆœì„œ ìš”ì•½

### 15.1 Phase 1 (N-gram) â€” ì¦‰ì‹œ ì‹œì‘

1. `spec_decode/config.py` â€” SpecDecodeConfig + CLI args
2. `spec_decode/proposer/base.py` â€” BaseProposer + factory
3. `spec_decode/proposer/ngram.py` â€” NGramProposer
4. `spec_decode/rejection_sampler.py` â€” NGramVerifier (greedy)
5. `spec_decode/verifier.py` â€” BatchedVerifier (ê¸°ì¡´ forward í™•ì¥)
6. `spec_decode/dynamic_controller.py` â€” DynamicSpecController
7. `spec_decode/kv_manager.py` â€” SpecDecodeKVManager
8. `engine/engine.py` ìˆ˜ì • â€” step()ì— spec decode ì‚½ì…
9. `serve.py` ìˆ˜ì • â€” CLI args + /metrics endpoint
10. í…ŒìŠ¤íŠ¸

### 15.2 Phase 2 (Draft Model) â€” Phase 1 ì•ˆì •í™” í›„

1. `spec_decode/proposer/draft_model.py` â€” DraftModelProposer
2. Draft model ë¡œë”© (mlx-lmì˜ `load`)
3. Draft model KV cache ê´€ë¦¬
4. `rejection_sampler.py`ì˜ `BatchedRejectionSampler` í™œì„±í™”
5. Engineì—ì„œ draft model forward ì§€ì›
6. í…ŒìŠ¤íŠ¸

### 15.3 Phase 3 (EAGLE) â€” Phase 2 ì™„ë£Œ í›„

1. `spec_decode/proposer/eagle.py` â€” EAGLEHead + EAGLEProposer
2. Target modelì—ì„œ hidden state ë°˜í™˜ ì¸í„°í˜ì´ìŠ¤ ì¶”ê°€
3. EAGLE head í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
4. í…ŒìŠ¤íŠ¸

### 15.4 Phase 4 (Dynamic Control ê³ ë„í™”) â€” ì „ Phaseì™€ ë³‘í–‰

1. Adaptive k êµ¬í˜„ (ì´ë¯¸ ê¸°ë³¸ êµ¬ì¡° ìˆìŒ)
2. Per-request override ì§€ì›
3. /metrics endpoint í™•ì¥
4. ë¶„ì‚° ì¶”ë¡  í™˜ê²½ spec decode ì¡°ìœ¨ (Mac Studio cluster)
